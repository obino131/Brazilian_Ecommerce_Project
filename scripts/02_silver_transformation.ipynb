{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa944d58-6969-4de9-8360-0d8e542896b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "612e5bb1-e40c-488e-80d2-3e3598bca18c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Configuration\n",
    "catalog = \"olist_project\"\n",
    "spark.sql(f\"USE CATALOG {catalog}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "185058a2-af1b-4d82-97c1-4d202feb1038",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Function to clean timestamps (Standardizing common Olist columns)\n",
    "def clean_olist_dates(df):\n",
    "    date_cols = [c for c in df.columns if \"timestamp\" in c or \"date\" in c]\n",
    "    for col in date_cols:\n",
    "        # Format for Olist is usually YYYY-MM-DD HH:MM:SS\n",
    "        df = df.withColumn(col, F.to_timestamp(F.col(col), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35bd3e55-266d-42d2-ba67-0a69be4f4cfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "CLEANING ORDERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b3d6799-183e-4410-872a-86e7045ed842",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"ðŸ§¼ Cleaning olist_orders...\")\n",
    "orders_df = spark.read.table(\"bronze.bronze_orders\")\n",
    "# Apply cleaning\n",
    "orders_clean = clean_olist_dates(orders_df)\n",
    "orders_clean = orders_clean.dropDuplicates([\"order_id\"]) # Deduplicate\n",
    "# Save to Silver\n",
    "orders_clean.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver.orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fe2f2a2-7478-42df-96ba-e87ed4f9260d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "CLEANING CUSTOMERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "283a02e9-7efe-41d6-807e-ac8a8688561a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"ðŸ§¼ Cleaning olist_customers...\")\n",
    "customers_df = spark.read.table(\"bronze.bronze_customers\")\n",
    "customers_clean = customers_df.dropDuplicates([\"customer_id\"])\n",
    "customers_clean.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver.customers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "717881ae-ed59-477e-9b87-499e979781f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ENRICHMENT: JOIN ORDERS + CUSTOMERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edb6230b-d215-405f-adbe-ccc56a283e14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"ðŸ”— Creating enriched_orders...\")\n",
    "enriched_orders = orders_clean.join(customers_clean, on=\"customer_id\", how=\"left\")\n",
    "\n",
    "# Save as a refined Silver table\n",
    "enriched_orders.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver.enriched_orders\")\n",
    "\n",
    "print(\"âœ… Silver Layer Transformation Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbbca4b2-20a4-49ac-9ddd-7eff85bfdf42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1. Configuration\n",
    "catalog = \"olist_project\"\n",
    "spark.sql(f\"USE CATALOG {catalog}\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS silver\")\n",
    "\n",
    "# Get list of all tables in Bronze\n",
    "tables = spark.catalog.listTables(\"bronze\")\n",
    "\n",
    "# 2. Generic Cleaning Function\n",
    "def clean_table(df, table_name):\n",
    "    # A. Standardize Column Names (lower case)\n",
    "    df = df.toDF(*[c.lower() for c in df.columns])\n",
    "    \n",
    "    # B. Fix Timestamps\n",
    "    # Any column with 'timestamp' or 'date' in the name gets converted\n",
    "    date_cols = [c for c in df.columns if \"timestamp\" in c or \"date\" in c]\n",
    "    for col in date_cols:\n",
    "        df = df.withColumn(col, F.to_timestamp(F.col(col), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    \n",
    "    # C. Basic Deduplication\n",
    "    # We try to guess the ID column (e.g., 'order_id' for 'orders')\n",
    "    id_col = [c for c in df.columns if \"_id\" in c and table_name.split(\"_\")[0] in c]\n",
    "    if id_col:\n",
    "        df = df.dropDuplicates([id_col[0]])\n",
    "        \n",
    "    return df\n",
    "\n",
    "# 3. Automation Loop\n",
    "for t in tables:\n",
    "    source_table = f\"bronze.{t.name}\"\n",
    "    target_table = f\"silver.{t.name}\"\n",
    "    \n",
    "    print(f\"ðŸ§¹ Cleaning {source_table} -> {target_table}...\")\n",
    "    \n",
    "    # Read, Clean, Write\n",
    "    df_raw = spark.read.table(source_table)\n",
    "    df_clean = clean_table(df_raw, t.name)\n",
    "    \n",
    "    df_clean.write.format(\"delta\").mode(\"overwrite\").saveAsTable(target_table)\n",
    "\n",
    "# 4. Create the \"Enriched\" Master Table (Special Business Step)\n",
    "# This joins orders, customers, and items into one big view for Gold layer prep\n",
    "print(\"ðŸ”— Creating silver.enriched_orders...\")\n",
    "orders = spark.read.table(\"silver.orders\")\n",
    "customers = spark.read.table(\"silver.customers\")\n",
    "items = spark.read.table(\"silver.order_items\")\n",
    "\n",
    "enriched = orders.join(customers, \"customer_id\", \"left\") \\\n",
    "                 .join(items, \"order_id\", \"left\")\n",
    "\n",
    "enriched.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver.enriched_orders\")\n",
    "\n",
    "print(\"âœ… All tables are now in Silver!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab43ef97-b173-4b84-8b42-be454735146f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 11"
    }
   },
   "outputs": [],
   "source": [
    "for t in spark.catalog.listTables(\"silver\"):\n",
    "    if t.name.startswith(\"bronze_\"):\n",
    "        new_name = t.name.replace(\"bronze_\", \"silver_\", 1)\n",
    "        spark.sql(f\"ALTER TABLE silver.{t.name} RENAME TO silver.{new_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27261b65-ae7a-4e4e-8482-4d3a93faff2e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 12"
    }
   },
   "outputs": [],
   "source": [
    "existing_tables = {t.name for t in spark.catalog.listTables(\"silver\")}\n",
    "for t in spark.catalog.listTables(\"silver\"):\n",
    "    if not t.name.startswith(\"silver_\"):\n",
    "        new_name = f\"silver_{t.name}\"\n",
    "        if new_name not in existing_tables:\n",
    "            spark.sql(f\"ALTER TABLE silver.{t.name} RENAME TO silver.{new_name}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_silver_transformation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
